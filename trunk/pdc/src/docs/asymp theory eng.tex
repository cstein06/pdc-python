\documentclass[a4paper,10pt]{article}

\usepackage[brazilian]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

%opening
\title{Asymptotic statistics for linear connectivity measures}
\author{Carlos Stein}

\begin{document}

\maketitle

\begin{abstract}

\end{abstract}

\section{Introduction}

This work is related to functional connectivity in the frequency domain based on multivariate linear models. For a more elaborated introduction, refer to ([1] Partial directed coherence: a new concept in neural structure determination).

Given $n$ temporal series, we desire to infer if there is a causal relation between them. For that, we will use the definition of Granger causality, so that we infer if one time series is helpful on the prediction of another time series. We will assume that the time series are generated by a multivariate autoregressive model (MVAR) of finite or $p$, expressed by:
\begin{equation}
x_{t} = \sum_{k=1}^{p}{A_{k}\cdot x_{t-k}} + \varepsilon_{t}; (n \times 1)
\end{equation}

where $x_{t}$  is the value of the multivariate series at time $t$, $A_k$, are the linear relationship matrices between the time series, and $\varepsilon_{t}$ are gaussian white noises, also interpreted as innovation processes.

A time series of index $j$ Granger-causes another of index $i$ if for any of the $A_k$ matrices we have $A_{k_{ij}} \neq 0$.

Based on this model we can define some connectivity indexes, each with their own properties and limitation. One measure, named Partial Directed Coherence (PDC), is defined in each frequency $\lambda$ by:
\begin{equation}
|\pi_{ij}(\lambda)|^{2} = \frac{a_{ij} \bar{a}_{ij}}{\sum_{k=1}^{n}{a_{kj} \bar{a}_{kj}}}  
\end{equation}

where each $a_{ij}$ is in the frequency domain, obtained by the discrete Fourier transform at frequency $\lambda$ of $a_{k_{ij}}$ for all $k$. This measure normalizes the relation from $j$ to $i$ by the other relations with $j$ as origin. In other words, it normalizes by the source.

When applying this analysis to real data, we must infer the values of $A$ and it becomes necessary to test the connectivity against the null hypothesis $ |\pi_{ij}(\lambda)|^{2} = 0$, and also calculate confidence ranges. This work obtains the asymptotic formulae for this. For previous related work, where the statistical properties for one of these measures is obtain, refer to ([2] Connectivity Inference Between Neural Structures via Partial Directed Coherence).

\section{Example of reference: diagonal PDC}

We will use the diagonal PDC (dPDC) (defined previously in the literature as \emph{generalized}, gPDC) as examples of the calculation procedures of the null hypothesis test and confidence range.
\begin{equation}
|\pi_{d_{ij}}(\lambda)|^{2} = \frac{a_{ij} \bar{a}_{ij} \sigma_{ij}^{-2}} {\sum_{k=1}^{n}{a_{kj} \bar{a}_{kj} \sigma_{kk}^{-2}}}
\end{equation}

The procedure is based widely on the delta method for asymptotic statistics. Given an estimator $\hat{y}$ for $y$, with asymptotic variance $\Omega/N$, and a function $g = g(\hat{y})$, the asymptotic variance of $g$ is $\frac{1}{N} \cdot G \cdot \Omega \cdot G^{'}$, where $G = \frac{\partial g(y)}{\partial y}$:
\begin{equation} 
\sqrt{N} (g(\hat{y}) - g(y)) \to N(0, G \Omega G^{'})
\end{equation}

more information on this theory can be found in the book [3] Asymptotic statistics - Van der Vaart.

If we have the asymptotic variance of the MVAR parameter estimators, we must calculate the derivative of our connectivity measure by these parameters. From the asymptotic variance we can derive the confidence range.

\subsection{Asymptotic variance of the MVAR estimators}

These calculations follow the theory expressed in the book [4] Introduction to multiple time series analysis - Helmut LÃ¼tkepohl.

Given a sample $X$ of size $N$ of the multivariate time series and an order $p$, we estimate the $A$ matrices, having $\varepsilon_{t}$ as model residues (or process innovations). The efficient estimators of $A$, for example Yule-Walker and Nuttall-Strand estimators, have as asymptotic variance:
\begin{equation}
\sqrt{N} (\hat{\alpha} - \alpha) \to N(0,\Omega_{\alpha}); (n^{2}p \times n^{2}p)
\end{equation}
where $\alpha = vec(A^{\ast})$ is how we represent the $A$ matrices jointly, $\Omega_{\alpha} = \Gamma_{x}(0)^{-1}\otimes \Sigma$ and $\Sigma = E[\varepsilon_{t}\cdot \varepsilon_{t}^{'}]$. $\Gamma_{x}(0)$ is defined by:
\begin{equation}
\Gamma_{x}(0) = \left[ \begin{array}{cccc} 
\Gamma(0) & \Gamma(1) & \cdots & \Gamma(p-1) \\
\Gamma(-1) & \Gamma(0) & \cdots & \Gamma(p-2) \\
\vdots & \vdots & \ddots & \vdots \\
\Gamma(p-1) & \Gamma(p-2) & \cdots & \Gamma(0) \end{array} \right]; (np \times np)
\end{equation}
where $\Gamma(k) = E[x_{t} \cdot x_{t-k}^{'}]$ and is estimated by the correlation between the time series and itself with lag $k$.

If $\Sigma$ is known, we have to estimate it too, based on the covariance of the estimated model residues. The asymptotic variance of this estimator is given by:
\begin{equation} 
\sqrt(N) (\hat{\varepsilon} - \varepsilon) \to N(0, \Omega_{\varepsilon})
\end{equation}
\begin{equation}
\Omega_{\varepsilon} = 2 \cdot D_{n} \cdot D_{n}^{+} \cdot (\Sigma \otimes \Sigma) \cdot D_{n}^{+^{'}} \cdot D_{n}^{'}
\end{equation}
where $\hat{\varepsilon} = vec(\hat{\Sigma})$ is how we represent $\Sigma$, $D_{n}$ is the duplication matrix for symmetric $ n \times n$ matrices $A$: $D_{n}*vech(A) = vec(A)$.

An important note is that the estimation of $\alpha$ and $\varepsilon$ are asymptotically independent.

\subsection{Matrix formulation}

We chose to develop the calculations in the matrix form, so here we rewrite the measure dPDC.

In the dPDC formula, we have variables in the frequency domain. They are obtained by:
\begin{equation}
a(\lambda)_{ij} = \sum_{k = 0}^{p} a_{ij} \cdot e^{-j2 \pi \lambda k}
\end{equation}
or in matrix form: $a^{\ast}(\lambda) = vec(I_{n} - \sum_{k=1}^{p} {A_{k} \cdot e^{-j2\pi\lambda k}})$. As $a^{\ast}(\lambda)$ is an imaginary vector, we will decompose it in its real and imaginary parts in order facilitate future derivatives: $a(\lambda) = \left[ \begin{array}{c} \Re(a^{\ast}(\lambda)) \\ \Im(a^{\ast}(\lambda)) \end{array} \right]$, which from now on will be denoted simply by $a$.

The matrix formulation of $a$ is:
$a = vec(\left[ I_{n} O_{n} \right]) - \zeta^{\ast} \cdot \alpha$, where $\zeta^{\ast}$ is the matrix that applies the Fourier transform, separating the real and imaginary parts ($\cos e \sin$) and can be constructed by:
\begin{equation}
C = [\cos(-2\pi\lambda \cdot 1) \cos(-2\pi\lambda \cdot 2) \dots \cos(-2\pi\lambda \cdot p)]
\end{equation}
\begin{equation}
S = \left[\sin(-2\pi\lambda \cdot 1) \sin(-2\pi\lambda \cdot 2) \dots \sin(-2\pi\lambda \cdot p)\right]
\end{equation}
\begin{equation}
\zeta = \left[\begin{array}{c} C \\ S \end{array} \right]
\end{equation}
\begin{equation}
\zeta^{\ast} = \zeta \otimes I_{n^{2}}
\end{equation}

Rewriting the dPDC:
\begin{equation} 
|\pi_{d_{ij}}(\lambda)|^{2} = \frac{a_{ij} \bar{a}_{ij} \sigma_{ij}^{-2}} {\sum_{k=1}^{n}{a_{kj} \bar{a}_{kj} \sigma_{kk}^{-2}}} = \frac{a^{'} \bar{\Sigma}_{d}^{-1} I_{ij}^{c} a}{a^{'} \bar{\Sigma}_{d}^{-1} I_{j}^{c} a}
\end{equation}

In the second formulation, $I_{ij}^{c}$ serves to select the $a(\lambda)_{ij}$ terms (both the real and imaginary parts), since $a_{ij} \bar {a}_{ij} = \Re{a_{ij}}^2 + \Im{a_{ij}}^2$, so that $I_{ij}^{c} \cdot a$ keeps only these elements and discards the others. $I_{j}^{c}$ acts similarly to select the whole column $j$ of $a$, discarding other terms. They can be defined by:
\begin{equation}
I_{ij} = diag(\left[ 0 \dots 0 1 0 \dots 0 \right]); (n^{2} \times n^{2})
\end{equation} 
with only $i+n\cdot j$ not zero.
\begin{equation}
I_{ij}^{c} = I_{2} \otimes I_{ij}; (2n^{2} \times 2n^{2}) 
\end{equation}
\begin{equation}
I_{j}^{\ast} = diag([0 \dots 0 1 0 \dots 0]); (n \times n)
\end{equation}
with only $j$ not zero.
\begin{equation}
I_{j} = I_{j}^{\ast} \otimes I_{n}; (n^{2} \times n^{2})
\end{equation}
\begin{equation}
I_{j}^{c} = I_{2} \otimes I_{j}; (2n^{2} \times 2n^{2})
\end{equation}

The matrix $\bar{\Sigma}_{d}$ is formulated conveniently to maintain the desired formulation:

\begin{equation}
\Sigma_{d} = diag(diag(\Sigma)); (n \times n)
\end{equation}
\begin{equation}
\bar{\Sigma}_{d} = I_{2n} \otimes \Sigma_{d}; (2n^{2} \times 2n^{2})
\end{equation}

The most important is that the formulation is given by the ratio of quadratic forms, facilitating the following calculations.

\subsection{Derivative of dPDC by $\alpha$}

When derivating dPDC by $a$, we have:
\begin{equation}
G_{d_1}^{\ast} = \frac{\partial |\pi_{d_{ij}}(\lambda)|^{2}}{\partial a^{'}} = \frac{2 a^{'} \bar{\Sigma}_{d}^{-1} I_{ij}^{c}} {a^{'} \bar{\Sigma}_{d}^{-1} I_{j}^{c} a} - \frac{2 a^{'} \bar{\Sigma}_{d}^{-1} I_{j}^{c} (a^{'} \bar{\Sigma}_{d}^{-1} I_{ij}^{c} a)} {(a^{'} \bar{\Sigma}_{d}^{-1} I_{j}^{c} a)^{2}}; (1 \times 2n^{2})
\end{equation}

The derivative of $a$ by $\alpha$ is given by:
\begin{equation}
\frac{\partial a}{\partial \alpha^{'}} = -\zeta^{\ast}; (2n^{2} \times n^{2}p)
\end{equation}

So the derivative of dPDC by $\alpha$, by the chain rule, is:
\begin{equation}
G_{d_1} = \frac{\partial |\pi_{d_{ij}}(\lambda)|^{2}}{\partial \alpha^{'}} = G_{d_1}^{\ast} \cdot -\zeta^{\ast}; (1 \times n^{2}p)
\end{equation}

So, if $\Sigma$ is known, the asymptotic variance of $|\hat{\pi}_{d_{ij}}(\lambda)|^{2}$ is:
\begin{equation}
\sqrt{N} (|\hat{\pi}_{d_{ij}}(\lambda)|^{2} - |\pi_{d_{ij}}(\lambda)|^{2}) \to N(0, G_{d_1} \Omega_{\alpha} G_{d_1}^{'}) 
\end{equation}

\subsection{Derivative of dPDC by $\varepsilon$}

If $\Sigma$ is not known, we have to estimate it. As it is part of the dPDC formula, its estimation influences the asymptotic variance of the estimator. We wish to find  $G_{d_e} = \frac{|\pi_{d_{ij}}(\lambda)|^{2}}{\partial \varepsilon^{'}}$.

Decomposing $|\pi_{d_{ij}}(\lambda)|^{2}$ in numerator and denominator:
\begin{equation}
|\pi_{d_{ij}}(\lambda)|^{2} = \frac{num_{d}}{den_{d}}
\end{equation}
\begin{equation}
num_{d} = a^{'} \bar{\Sigma}_{d}^{-1} I_{ij}^{c} a
\end{equation}
\begin{equation}
den_{d} = a^{'} \bar{\Sigma}_{d}^{-1} I_{j}^{c} a
\end{equation}
and define $\bar{\varepsilon} = vec(\bar{\Sigma}); (4n^{4} \times 1)$.

We apply the chain rule repeatedly:
\begin{equation}
\frac{|\pi_{d_{ij}}(\lambda)|^{2}}{\partial \varepsilon^{'}} = \frac{den_{d} \cdot \frac{\partial num_{d}}{\partial \varepsilon^{'}} - num_{d} \cdot \frac{\partial den_{d}}{\partial \varepsilon^{'}}}{den_{d}^{2}}; (1 \times \frac{n(n+1)}{2})
\end{equation}

Again the chain rule:
\begin{equation}
\frac{num_{d}}{\partial \varepsilon^{'}} = \frac{num_{d}}{\partial \bar{\varepsilon}^{'}} \cdot \frac{\partial \bar{\varepsilon}}{\partial \varepsilon^{'}}
\end{equation}
\begin{equation}
\frac{den_{d}}{\partial \varepsilon^{'}} = \frac{den_{d}}{\partial \bar{\varepsilon}^{'}} \cdot \frac{\partial \bar{\varepsilon}}{\partial \varepsilon^{'}}
\end{equation}

We have:
\begin{equation}
\frac{\partial num_{d}}{\partial \bar{\varepsilon}^{'}} = ((I_{ij} \cdot a)^{'} \otimes a^{'}) \cdot \frac{\partial vec(\bar{\Sigma}_{d}^{-1})}{\partial \bar{\varepsilon}^{'}}
\end{equation}
\begin{equation}
\frac{\partial den_{d}}{\partial \bar{\varepsilon}^{'}} = ((I_{j} \cdot a)^{'} \otimes a^{'}) \cdot \frac{\partial vec(\bar{\Sigma}_{d}^{-1})}{\partial \bar{\varepsilon}^{'}}
\end{equation}
and
\begin{equation}
\frac{\partial vec(\bar{\Sigma}_{d}^{-1})}{\partial \bar{\varepsilon}^{'}} = diag(vec(-\bar{\Sigma}_{d}^{2}))
\end{equation}
and it ignores terms out of the diagonal.

As $\bar{\varepsilon} = vec(I_{2n} \otimes \Sigma)$, we have
\begin{equation}
\frac{\partial \bar{\varepsilon}}{\partial \varepsilon^{'}} = (T_{2n,n} \otimes I_{n \cdot 2n}) \cdot (I_{n} \otimes vec(I_{2n}) \otimes I_{n}); (4n^{4} \times n^{2})
\end{equation}
where $T_{p,q} \cdot vec(B) = vec(B^{'})$, with $B (p \times q)$, concluding the derivative calculation.

OBS: The formula used is $\frac{\partial vec(A \otimes B)}{\partial vec(B)} = (T_{n,p} \otimes I_{n \cdot q}) \cdot (I_{q} \otimes vec(A) \otimes I_{p}); A (m \times n), B (p \times q)$.

So the asymptotic variance of the dPDC is the sum of the variances related to the estimation of $\alpha$ and $\varepsilon$:
\begin{equation}
\sqrt{N} (|\hat{\pi}_{d_{ij}}(\lambda)|^{2} - |\pi_{d_{ij}}(\lambda)|^{2}) \to N(0, G_{d_1} \Omega_{\alpha} G_{d_1}^{'} + G_{d_e} \Omega_{\varepsilon} G_{d_e}^{'}) 
\end{equation}

For confidence range all one have to do is calculate the respective quantile of the asymptotic normal distribution with this variance.

\subsection{Test against null hypothesis}

An important observation is that the variance obtained in the previous section is null under $H_0$, as $I_{j}^{c} a = 0$, so that we have to find the second order derivatives. In this case, given the estimator $\hat{y}$ of $y$, with asymptotic variance $\Omega$ so that $\frac{\Omega}{N} \to 0$, and a function $g = g(\hat{y})$, with $G_2 = \frac{\partial g(y)}{\partial y \partial y^{'}}$, we have:
\begin{equation}
N (g(\hat{y}) - g(y)) \to \frac{1}{2} \cdot x^{'} \cdot G_2 \cdot x
\end{equation}
with $x \to N(0, \Omega)$.

In this case we can make a variable transformation that transforms this limit in a sum of chi-squares. Let $\Omega = L \cdot L^{'}$ be the Choleski decomposition of $\Omega$, with $L$ a inferior triangular matrix. Define $x = Lz$, so $z = (L^{'}L)^{-1} L^{'} x$.

We have that $E[z \cdot z^{'}] = I$. So if $D = L^{'} G_2 L$ and $D = U \Lambda U^{'}$ its diagonalization, we have:
\begin{equation}
x^{'} G_{2} x = z^{'} D z = \sum_{k} {l_{k} z^{'} u_{k} u_{k}^{'} z} = \sum_{k} {l_{k} \nu_{k}^{2}}
\end{equation}
as $u_{k} u_{k}^{'} = I$, with $l_{k}$ being the eigenvalues of $D$, and $\nu_{k}^{2}$ chi-squares variables with one degree of freedom.

Thus $N (g(\hat{y}) - g(y)) \to \sum_{k} {l_{k} \nu_{k}^{2}}$.

Now in order to have a second order asymptotic distribution of our measures, we need the second derivative $G_{d_2}$, given by:
\begin{equation}
G_{d_2} = \frac{\partial |\pi_{d_{ij}}(\lambda)|^{2}}{\partial \alpha \partial \alpha^{'}} = \zeta^{\ast^{'}} \cdot G_{2}^{\ast} \cdot \zeta^{\ast}; (n^{2}p \times n^{2}p)
\end{equation}
\begin{equation}
G_{d_2}^{\ast} = \frac{\partial |\pi_{d_{ij}}(\lambda)|^{2}}{\partial a \partial a^{'}} = \frac{2 \bar{\Sigma}_{d}^{-1} I_{ij}^{c}}{(a^{'} \bar{\Sigma}_{d}^{-1} I_{j}^{c} a)}; (2n^{2} \times 2n^{2})
\end{equation}

Given $G_{d_2}$, you have to decompose $\Omega$, calculate $D$ and find its eigenvalues. For the hypothesis test, you calculate the superior quantile of the chi-square sum $\sum_{k} {l_{k} \nu_{k}^{2}}$. 

For the calculation of the quantile of a sum of chi-squares, we use the Patnaik approximation, approximating by one chi-square: $c\cdot \chi^2_d$, with $c = \frac{\sum_k {l_k^2}}{(\sum_k {l_k})^2}$ and $d = \frac{\sum_k {l_k^2}}{(\sum_k {l_k})^2}$.

We have $rank(D) \le rank(G_{2}^{\ast} \le rank(I_{ij}^{c}) = 2$, so at most two eigenvalues with be non-zero, being a sum of at most two chi-squares.

In case $\Sigma$ is unknown, we must add the variance relative to its estimation. In the case of dPDC, this derivative is zero, as the $I_{ij}^{c} a$ terms, that are zero under $H_0$, don't disappear with the derivative.

\section{Statistics for PDC, gPDC and DTF}

Using the same procedure as in the previous section, we will calculate the statistics for other measures of connectivity based on the MVAR model.

\subsection{PDC}

The Partial Directed Coherence (PDC) is the original measure proposed by Bacalla and Sameshima, and did not have a normalization by the variance of the residues:
\begin{equation}
|\pi_{ij}(\lambda)|^{2} = \frac{a_{ij} \bar{a}_{ij}}{\sum_{k=1}^{n}{a_{kj} \bar{a}_{kj}}} = \frac{a^{'} I_{ij}^{c} a}{a^{'} I_{j}^{c} a}
\end{equation}

It is a formulation similar to dPDC, but without the presence of $\Sigma$. This way we have similar derivatives:
\begin{equation}
G_{1}^{\ast} = \frac{\partial |\pi_{ij}(\lambda)|^{2}}{\partial a^{'}} = \frac{2 a^{'} I_{ij}^{c}} {a^{'} I_{j}^{c} a} - \frac{2 a^{'} I_{j}^{c} (a^{'} I_{ij}^{c} a)} {(a^{'} I_{j}^{c} a)^{2}}; (1 \times 2n^{2})
\end{equation}
and under $H_0$ ($I_{ij}^{c} a = 0$):
\begin{equation}
G_{2}^{\ast} = \frac{\partial |\pi_{ij}(\lambda)|^{2}}{\partial a \partial a^{'}} = \frac{2 I_{ij}^{c}}{(a^{'} I_{j}^{c} a)}; (2n^{2} \times 2n^{2})
\end{equation}
instead of $G_{d_1}$ and $G_{d_2}$, respectively.

As there is no dependence of $\hat{\Sigma}$ in its definition, we do not need to consider its asymptotic variance, or consider the derivative to $\varepsilon$ as zero.

\subsection{gPDC}

The generalized PDC (gPDC) is different from dPDC by having a normalization by the full $\Sigma$ matrix, not only its diagonal:

\begin{equation}
 |\pi_{g_{ij}}(\lambda)|^{2}  = \frac{a^{'} \bar{\Sigma}_{d}^{-1} I_{ij}^{c} a}{a^{'} I_{j}^{c} \bar{\Sigma}^{-1} I_{j}^{c} a}
\end{equation}
with $\bar{\Sigma} = I_{2n} \otimes \Sigma$.

Again its formulation is similar, with derivatives by $a$ as:
\begin{equation}
G_{g_1}^{\ast} = \frac{\partial |\pi_{g_{ij}}(\lambda)|^{2}}{\partial a^{'}} = \frac{2 a^{'} \bar{\Sigma}_{d}^{-1} I_{ij}^{c}} {a^{'} I_{j}^{c} \bar{\Sigma}^{-1} I_{j}^{c} a} - \frac{2 a^{'} I_{j}^{c} \bar{\Sigma}^{-1} I_{j}^{c} (a^{'} \bar{\Sigma}_{d}^{-1} I_{ij}^{c} a)} {(a^{'} I_{j}^{c} \bar{\Sigma}^{-1} I_{j}^{c} a)^{2}}; (1 \times 2n^{2})
\end{equation}
\begin{equation}
G_{g_2}^{\ast} = \frac{\partial |\pi_{g_{ij}}(\lambda)|^{2}}{\partial a \partial a^{'}} = \frac{2 \bar{\Sigma}_{d}^{-1} I_{ij}^{c}}{(a^{'} I_{j}^{c} \bar{\Sigma}^{-1} I_{j}^{c} a)}; (2n^{2} \times 2n^{2})
\end{equation}
and derivatives by $\varepsilon$ as:
\begin{equation}
\frac{\partial den_{g}}{\partial \bar{\varepsilon}^{'}} = ((I_{j} \cdot a)^{'} \otimes (a^{'} \cdot I_{j})) \cdot \frac{\partial vec(\bar{\Sigma}^{-1})}{\partial \bar{\varepsilon}^{'}}
\end{equation}
with:
\begin{equation}
\frac{\partial vec(\bar{\Sigma}^{-1})}{\partial vec(\bar{\Sigma})^{'}} = -\bar{\Sigma}^{-1^{'}} \otimes \bar{\Sigma}^{-1}
\end{equation}

\subsection{DTF}

Another group of connectivity measure uses the inverse of the matrix $A(\lambda)$. One of the is the directed transfer function (DTF):
\begin{equation}
 |dtf_{ij}(\lambda)|^{2} = \frac{h^{'} I_{ij}^{l} h}{h^{'} I_{i}^{l} h}
\end{equation}
where $h = vec([\begin{array}{cc}\Re(A^{-1}(\lambda)) & \Im(A^{-1}(\lambda))])\end{array}$, as with $a$. The matrix $I_{i}^{l}$ selects the line $i$ (differently of $I_{j}^{c}$ that selects the column $j$):
\begin{equation}
I_{i}^{\ast} = diag([0 \dots 0 1 0 \dots 0]); (n \times n)
\end{equation}
with only $i$ not zero.
\begin{equation}
I_{i} = I_{n} \otimes I_{i}^{\ast}; (n^{2} \times n^{2})
\end{equation}
\begin{equation}
I_{i}^{l} = I_{2} \otimes I_{i}; (2n^{2} \times 2n^{2})
\end{equation}

We see that the formulation is analogous to the PDC formulation, but having $h$ instead of $a$ and $I_{i}^{l}$ instead of $I_{j}^{c}$. This way we add in the chain rules the derivative $\frac{\partial h}{\partial a^{'}}$ and substitute $I_{i}^{l}$ by $I_{j}^{c}$:

\begin{equation}
G_{dtf_{1}}^{\ast} = \frac{\partial |dtf_{ij}(\lambda)|^{2}} {\partial h^{'}} = \frac{2 h^{'} I_{ij}^{l}} {h^{'} I_{i}^{l} h} - \frac{2 h^{'} I_{i}^{l} (h^{'} I_{ij}^{l} h)} {(h^{'} I_{i}^{l} h)^{2}}; (1 \times 2n^{2})
\end{equation}
\begin{equation}
G_{dtf_{1}} = G_{dtf_{1}}^{\ast} \cdot \frac{\partial h}{\partial a^{'}} \cdot -\zeta^{\ast} 
\end{equation}
\begin{equation}
G_{dtf_{2}}^{\ast} = \frac{\partial |dtf_{ij}(\lambda)|^{2}} {\partial h \partial h^{'}} = \frac{2 I_{ij}^{l}}{(h^{'} I_{j}^{l} h)}; (2n^{2} \times 2n^{2})
\end{equation}
\begin{equation}
G_{dtf_{2}} = \zeta^{\ast^{'}} \cdot (\frac{\partial h}{\partial a^{'}})^{'} \cdot G_{dtf_{2}}^{\ast} \cdot \frac{\partial h}{\partial a^{'}} \cdot \zeta^{\ast}
\end{equation}

For the calculation of  $\frac{\partial h}{\partial a^{'}}$ we have $H = A^{-1}$, so the complex derivative will be:
\begin{equation}
B = \frac{\partial vec(H)}{\partial vec(A)^{'}} = -H^{'} \otimes H
\end{equation}

Decomposing in real and imaginary parts:
\begin{equation}
\frac{\partial h}{\partial a^{'}} = \left[ \begin{array}{cc} \Re(B) & -\Im(B) \\ \Im(B) & \Re(B)
\end{array} \right]
\end{equation}

\section{Coherence, partial coherence and spectral density}

\subsection{Partial coherence}

The partial coherence in matrix form is defined by:
\begin{equation}
PC_{num_{ij}} = \left( a^{'} k_{1}^{ij} a \right)^{2} + \left( a^{'} k_{2}^{ij} a \right)^{2}
\end{equation}
\begin{equation}
PC_{den_{ij}} = \left( a^{'} k_{1}^{ii} a \right) \cdot \left( a^{'} k_{1}^{jj} a \right)
\end{equation}
\begin{equation}
PC_{ij} = \frac{PC_{num_{ij}}}{PC_{den_{ij}}}
\end{equation}
\begin{equation}
k_{1}^{ij} = \left[ \begin{array}{cc}
 c_{i} & 0 \\ 0 & c_{i}
\end{array} \right] \Sigma_{2}^{-1} \left[ \begin{array}{cc}
 c_{j} & 0 \\ 0 & c_{j}
\end{array} \right]^{'}
\end{equation}
\begin{equation}
k_{2}^{ij} = \left[ \begin{array}{cc}
 c_{i} & 0 \\ 0 & c_{i}
\end{array} \right] \Sigma_{2}^{-1} \left[ \begin{array}{cc}
 0 & c_{j} \\ -c_{j} & 0
\end{array} \right]^{'} 
\end{equation}
where $c_i$ selects the column $i$. 

The matrix $c_i$ is given by:

\begin{equation}
v_{i} = [0 \dots 0 1 0 \dots 0]^{'}; (n \times 1)
\end{equation}
with only $i$ not zero.
\begin{equation}
c_{i} = v_{i} \otimes I_{n}; (n^{2} \times n)
\end{equation}
and $\Sigma_{2} = I_{2} \otimes \Sigma$.

So we have the derivative respective to $a$:
\begin{equation}
G_{PC_1} = \frac{\partial PC_{ij}}{\partial a^{'}} = \frac{2 (a^{'} k_{1}^{ij} a) a^{'} (k_{1}^{ij} + k_{1}^{ij^{'}}) + 2 (a^{'} k_{2}^{ij} a) a^{'} (k_{2}^{ij} + k_{2}^{ij^{'}}) } 
{PC_{den_{ij}}} -
\end{equation}
\begin{equation}
\frac{PC_{num_{ij}}}{PC_{den_{ij}}^{2}} \cdot (2 a^{'} k_{1}^{ii} (a^{'} k_{1}^{jj} a) + 2 a^{'} k_{1}^{jj} ( a^{'} k_{1}^{ii} a))
\end{equation}
\begin{equation}
G_{PC_2} = \frac{\partial PC_{ij}}{\partial a \partial a^{'}} = \frac{2}{PC_{den_{ij}}} \cdot ((k_{1}^{ij} + k_{1}^{ij^{'}}) a a^{'} (k_{1}^{ij} + k_{1}^{ij^{'}}) + (k_{2}^{ij} + k_{2}^{ij^{'}}) a a^{'} (k_{2}^{ij} + k_{2}^{ij^{'}}))
\end{equation}

For the derivative in respect to $\varepsilon$, we rewrite PC as:
\begin{equation}
a_{11}^{i} = a^{'} \left[ \begin{array}{cc}
 c_{i} & 0 \\ 0 c_{i}
\end{array} \right]
\end{equation}
\begin{equation}
a_{12}^{i} = \left[ \begin{array}{cc}
 c_{i} & 0 \\ 0 & c_{i}
\end{array} \right]^{'} a
\end{equation}
\begin{equation}
a_{21}^{i} = a^{'} \left[ \begin{array}{cc}
 c_{i} & 0 \\ 0 c_{i}
\end{array} \right]
\end{equation}
\begin{equation}
a_{22}^{i} = \left[ \begin{array}{cc}
 0 & c_{i} \\ -c_{i} & 0
\end{array} \right]^{'} a
\end{equation}
\begin{equation}
num_{PC} = (a_{11}^{i} \Sigma_{2}^{-1} a_{12}^{j})^{2} + (a_{21}^{i} \Sigma_{2}^{-1} a_{22}^{j})^{2}
\end{equation}
\begin{equation}
den_{PC_{1}} = a_{11}^{i} \Sigma_{2}^{-1} a_{12}^{i}
\end{equation}
\begin{equation}
den_{PC_{2}} = a_{11}^{j} \Sigma_{2}^{-1} a_{12}^{j}
\end{equation}
\begin{equation}
PC_{ij} = \frac{num_{PC}}{den_{PC_{1}} \cdot den_{PC_{2}}}
\end{equation}

so that by the chain rule:
\begin{equation}
\frac{\partial PC_{ij}}{\partial \varepsilon^{'}} = \frac{\frac{\partial num_{PC}}{\partial \varepsilon^{'}}}{den_{PC_{1}} \cdot den_{PC_{2}}} -
\frac{num_{PC}}{den_{PC_{1}} \cdot den_{PC_{2}}} \left( \frac{1}{den_{PC_{2}}} \frac{\partial den_{PC_{2}}}{\partial \varepsilon^{'}} +
 \frac{1}{den_{PC_{1}}} \frac{\partial den_{PC_{1}}}{\partial \varepsilon^{'}}  \right) 
\end{equation}

Let $\varepsilon_{2} = vec(\Sigma_{2})$, we have:
\begin{equation}
\frac{\partial num_{PC}}{\partial \varepsilon^{'}} = (2 (a_{11}^{i} \Sigma_{2}^{-1} a_{12}^{j}) (a_{12}^{j^{'}} \otimes a_{11}^{i}) +
2 (a_{21}^{i} \Sigma_{2}^{-1} a_{22}^{j}) (a_{22}^{j^{'}} \otimes a_{21}^{i}) )
\frac{\partial vec(\Sigma_{2}^{-1})}{\partial \varepsilon^{'}}
\end{equation}
\begin{equation}
\frac{\partial den_{PC_{1}}}{\partial \varepsilon^{'}} = (a_{12}^{i^{'}} \otimes a_{11}^{i}) 
\frac{\partial vec(\Sigma_{2}^{-1})}{\partial \varepsilon^{'}}
\end{equation}
\begin{equation}
\frac{\partial den_{PC_{2}}}{\partial \varepsilon^{'}} = (a_{12}^{j^{'}} \otimes a_{11}^{j}) )
\frac{\partial vec(\Sigma_{2}^{-1})}{\partial \varepsilon^{'}}
\end{equation}

As $\varepsilon_{2} = vec(I_{2} \otimes \Sigma)$, we have:
\begin{equation}
\frac{\partial \varepsilon_{2}}{\partial \varepsilon^{'}} = (T_{2,n} \otimes I_{n \cdot 2}) \cdot (I_{n} \otimes vec(I_{2}) \otimes I_{n}); (4n^{2} \times n^{2})
\end{equation}
and
\begin{equation}
\frac{\partial vec(\Sigma_{2}^{-1})}{\partial vec(\Sigma_{2})^{'}} = -\Sigma_{2}^{-1^{'}} \otimes \Sigma_{2}^{-1}
\end{equation}
thus:
\begin{equation}
\frac{\partial vec(\Sigma_{2}^{-1})}{\partial \varepsilon^{'}} = \frac{\partial vec(\Sigma_{2}^{-1})}{\partial vec(\Sigma_{2})^{'}} \cdot \frac{\partial \varepsilon_{2}}{\partial \varepsilon^{'}}
\end{equation}

Under $H_0$ we have $num_{PC} = 0$. This time the second derivative is not zero. First $\frac{\partial PC_{ij}}{\partial \varepsilon \partial \varepsilon^{'}}$:
\begin{equation}
\frac{\partial num_{PC_{1}}}{\partial \varepsilon^{'}} = \frac{\partial (a_{11}^{i} \Sigma_{2}^{-1} a_{12}^{j})}{\partial \varepsilon^{'}} = a_{12}^{j^{'}} \otimes a_{11}^{i} \frac{\partial vec(\Sigma_{2}^{-1})}{\partial \varepsilon^{'}}
\end{equation}
\begin{equation}
\frac{\partial num_{PC_{2}}}{\partial \varepsilon^{'}} = \frac{\partial (a_{21}^{i} \Sigma_{2}^{-1} a_{22}^{j})}{\partial \varepsilon^{'}} = a_{22}^{j^{'}} \otimes a_{21}^{i} \frac{\partial vec(\Sigma_{2}^{-1})}{\partial \varepsilon^{'}}
\end{equation}
\begin{equation}
\frac{\partial PC_{ij}}{\partial \varepsilon \partial \varepsilon^{'}} = \frac{2 \frac{\partial num_{PC_{1}}}{\partial \varepsilon} \frac{\partial num_{PC_{1}}}{\partial \varepsilon^{'}} + 2 \frac{\partial num_{PC_{2}}}{\partial \varepsilon} \frac{\partial num_{PC_{2}}}{\partial \varepsilon^{'}}}{den_{PC_{1}} \cdot den_{PC_{2}}}
\end{equation}

We also need $\frac{\partial PC_{ij}}{\partial \alpha \partial \varepsilon^{'}}$:
\begin{equation}
\frac{\partial PC_{ij}}{\partial \alpha \partial \varepsilon^{'}} = \frac{2 \frac{\partial num_{PC_{1}}}{\partial \alpha} \frac{\partial num_{PC_{1}}}{\partial \varepsilon^{'}} + 2 \frac{\partial num_{PC_{2}}}{\partial \alpha} \frac{\partial num_{PC_{2}}}{\partial \varepsilon^{'}}}{den_{PC_{1}} \cdot den_{PC_{2}}}
\end{equation}
where
\begin{equation}
\frac{\partial num_{PC_{1}}}{\partial \alpha} = (k_{1}^{ij} + k_{1}^{ij^{'}}) a \frac{\partial a}{\partial \alpha^{'}}
\end{equation}
\begin{equation}
\frac{\partial num_{PC_{2}}}{\partial \alpha} = (k_{2}^{ij} + k_{2}^{ij^{'}}) a \frac{\partial a}{\partial \alpha^{'}}
\end{equation}

Let $\Omega_{big} = \left[ \begin{array}{cc} 
 \Omega_{\alpha} & 0 \\ 0 & \Omega_{\varepsilon}
\end{array} \right]$, $par = \left[ \begin{array}{c} 
 \alpha \\ \varepsilon
\end{array} \right]$ e $G_{PC_{par}} = \frac{\partial PC_{ij}}{\partial par^{'}} = \left[ \begin{array}{cc} 
\frac{\partial PC_{ij}}{\partial \alpha \partial \alpha^{'}} & 
\frac{\partial PC_{ij}}{\partial \alpha \partial \varepsilon^{'}} \\ 
\frac{\partial PC_{ij}}{\partial \varepsilon \partial \alpha^{'}} & 
\frac{\partial PC_{ij}}{\partial \varepsilon \partial \varepsilon^{'}}
\end{array} \right]$, we have under $H_0$
\begin{equation}
N (\hat{PC_{ij}} - PC_{ij}) \to \frac{1}{2} \cdot x^{'} \cdot G_{PC_{par}} \cdot x
\end{equation}
with $x \to N(0, \Omega_{big})$.

So we must proceed with the same decomposition as for the $\Omega_{alpha}$, arriving at the sum of chi-squares.

\subsection{Coherence}

The confidence interval for the coherence was presented in a previous paper (Asymptotic simultaneous confidence bands for vector autoregressive spectra), but they have not considered what happened under $H_0$.

The coherence (coh) is defined analogously to the PC, but with the inverse matrix $H$, so that $h$ substitutes $a$ and $\Sigma_{2}$ substitutes $\Sigma_{2}^{-1}$:
\begin{equation}
coh_{num_{ij}} = \left( h^{'} k_{coh_{1}}^{ij} h \right)^{2} + \left( h^{'} k_{coh_{2}}^{ij} h \right)^{2}
\end{equation}
\begin{equation}
coh_{den_{ij}} = \left( h^{'} k_{coh_{1}}^{ii} h \right) \cdot \left( h^{'} k_{coh_{1}}^{jj} h \right)
\end{equation}
\begin{equation}
coh_{ij} = \frac{PC_{num_{ij}}}{PC_{den_{ij}}}
\end{equation}
\begin{equation}
k_{coh_{1}}^{ij} = \left[ \begin{array}{cc}
 l_{i} & 0 \\ 0 & l_{i}
\end{array} \right] \Sigma \left[ \begin{array}{cc}
 l_{j} & 0 \\ 0 & l_{j}
\end{array} \right]^{'}
\end{equation}
\begin{equation}
k_{coh_{2}}^{ij} = \left[ \begin{array}{cc}
 l_{i} & 0 \\ 0 & l_{i}
\end{array} \right] \Sigma \left[ \begin{array}{cc}
 0 & l_{j} \\ -l_{j} & 0
\end{array} \right]^{'}
\end{equation}
where $l_{i} = I_{n} \otimes v_{i}$ selects the line $i$ of $h$.

The rest of the calculations are analogous, only adding $\frac{\partial h}{\partial a^{'}}$ in the chain rule of the derivative in respect to $\alpha$, as was done with the DTF.

\subsection{Spectral density}

The spectral density (SS) is similar to the coherence, but without normalization:
\begin{equation}
SS_{ij} = \left( h^{'} k_{coh_{1}}^{ij} h \right)^{2} + \left( h^{'} k_{coh_2}^{ij} h \right)^{2}
\end{equation}

So the calculations are same, substituting $SS_{den}$ by 1.

\end{document}



